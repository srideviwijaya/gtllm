docker run --rm -it --name trtllm -v ${PWD}:/mnt -w /mnt --gpus all -p8000:8000 -p8002:8002 -p8001:8001 nvcr.io/nvidia/tritonserver:24.07-trtllm-python-py3

pip install tensorrt_llm -U --extra-index-url https://pypi.nvidia.com
pip install nvidia-pyindex
pip install nvidia-tensorrt

export HF_LLAMA_MODEL=/mnt/triton/model_repository/llamav2/1/f5db02db724555f92da89c216ac04704f23d4590
export UNIFIED_CKPT_PATH=/mnt/tensorrt/llama/ckpt/
export ENGINE_PATH=/mnt/tensorrt/llama/engines/

python3 /mnt/tensorrt/TensorRT-LLM/examples/llama/convert_checkpoint.py --model_dir ${HF_LLAMA_MODEL} --output_dir ${UNIFIED_CKPT_PATH} --dtype float16

trtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH} \
             --remove_input_padding enable \
             --gpt_attention_plugin float16 \
             --context_fmha enable \
             --gemm_plugin float16 \
             --output_dir ${ENGINE_PATH} \
             --paged_kv_cache enable \
             --max_batch_size 1
			

cd tensorrtllm_backend
cp all_models/inflight_batcher_llm/ llama_ifb -r 			
			 
python3 tools/fill_template.py -i llama_ifb/preprocessing/config.pbtxt tokenizer_dir:${HF_LLAMA_MODEL},triton_max_batch_size:4,preprocessing_instance_count:1
python3 tools/fill_template.py -i llama_ifb/tensorrt_llm_bls/config.pbtxt triton_max_batch_size:4,decoupled_mode:False,bls_instance_count:1,accumulate_tokens:False
python3 tools/fill_template.py -i llama_ifb/ensemble/config.pbtxt triton_max_batch_size:4
python3 tools/fill_template.py -i llama_ifb/tensorrt_llm/config.pbtxt triton_backend:tensorrtllm,triton_max_batch_size:4,decoupled_mode:False,max_beam_width:1,engine_dir:${ENGINE_PATH},max_tokens_in_paged_kv_cache:2560,max_attention_window_size:2560,kv_cache_free_gpu_mem_fraction:0.5,exclude_input_in_output:True,enable_kv_cache_reuse:False,batching_strategy:inflight_fused_batching,max_queue_delay_microseconds:0

python3 scripts/launch_triton_server.py --world_size 1 --model_repo=llama_ifb/ --http_port 8000